---
title: "Forecasting Model Similarity (for inc hosp)"
author: "Johannes Bracher, Evan Ray, Nick Reich, Nutcha Wattanachit, Li Shandross"
date: "06/28/2021"
header-includes:
   - \usepackage{tabularx}
   - \usepackage{hyperref}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{xcolor}
output:
  pdf_document:
        latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(energy)
library(knitr)
library(data.table)
library(covidHubUtils)
#devtools::install_github("reichlab/covidHubUtils")
library(lubridate)
library(zoltr)
library(igraph)
library(gtools)
library(gridExtra)
library(ggdendro)
knitr::opts_chunk$set(echo=FALSE,
                       comment = FALSE, message=FALSE, fig.show= 'hold',fig.pos="H",table.placement='H',
                       fig.align = 'center')
```


# COVID-19 Forecasting Model Similarity Analysis for 1-4 Week Ahead Incident Hospitalization 

In this analysis, we extend the work of Bracher, et. al that evaluates the similarity between Covid-19 models using Cramer's distance. This work was applied to forecasts for incident deaths (inc deaths) and incident cases (inc cases) and here we apply it to incident hospitalizations (inc hosp).

We must first make some adjustments to the inc hosp forecast data, which have a temporal resolution of "day" instead of "week," unlike inc case and inc death. This presents a challenge because the horizons will be different for the same target end date if the forecast dates between two models differ by only a single day. (This is not an issue when the temporal resolution is in terms of weeks, which are defined by epidemiological week, not the number of days between forecast date and target end date.) Thus, we create a new variable called horizon week to solve this issue. This variable counts horizons between 1 and 7 to have a horizon week of 1, horizons between 8 and 14 to have a horizon week of 2, etc. Hence, the analyses used on inc death and inc cases should be able to be applied easily to the inc hosp data.

This initial analysis only examines target end dates for a single day of the week, Thursday, to account for models that include day of the week effects. However, this will be expanded later to more days, perhaps even exploring if performing the analysis on different days of the week leads to different results in terms of model similarity.


The pairwise approximated Cramer's distances are calculated for the models that have complete submissions for all targets, all probability levels, and no missing forecasts between December 17th, 2020 and June 10th 2021. We aggregate results for the five locations that have the highest number of number of COVID-19 hospitalizations during this period as well as the five locations with the lowest number for the same date range.


```{r}
# rewrite frame_format() to work w/ hosp data
frame_format2 <- function(zoltr_frame){
  n_locs <- length(unique(zoltr_frame$location))
  # filter
  formatted_frame <- zoltr_frame %>%
    dplyr::filter(!any(is.na(value)),
                  !any(is.null(value))) %>%
    # filtering on quantile, which is the smallest
    dplyr::group_by(location, horizon_week, target_end_date, model) %>%
    mutate(n_q = n_distinct(quantile)) %>%
    ungroup() %>%
    dplyr::filter(n_q==max(n_q)) %>%
    dplyr::select(-"n_q") %>%
    # start filtering date and location and horizon
    group_by(model, horizon_week,  target_end_date) %>% #Add count of locations
    mutate(n_locations = n_distinct(location)) %>%
    dplyr::filter(n_locations==n_locs) %>%
    ungroup()  %>%
    group_by(model, location, target_end_date) %>% #Add count of weeks
    dplyr::mutate(n_horizons = n_distinct(horizon_week)) %>%
    ungroup() %>%
    dplyr::filter(n_horizons==max(n_horizons)) %>%
    group_by(model, horizon_week, location) %>%
    mutate(n_dates = n_distinct(target_end_date)) %>%
    ungroup() %>%
    dplyr::filter(n_dates==max(n_dates)) %>%
    dplyr::select(-c("n_horizons","n_locations","n_dates"))
  # final clean-up
  matrix_frame <- formatted_frame %>%
    dplyr::select("location","target_variable","target_end_date",
                  "type","quantile","model","value","horizon_week") %>%
    rename(horizon = `horizon_week`) %>% 
    dplyr::arrange(location,horizon,target_variable,target_end_date,model,quantile) %>%
    tidyr::pivot_wider(names_from = model, values_from = value) %>%
    dplyr::select_if(~ !any(is.na(.)))
  return(matrix_frame)
} 
```

```{r}
source("./functions/distance_func_script.R")
# set targets for analysis
target_horizon <- 1:4
target_var <- "inc hosp"
# read in model metadata
metadata <- read.csv("metadata_categorized.csv") 
metadata$stats[which(metadata$team_name == "Karlen Working Group")] <- TRUE
metadata$compartmental[which(metadata$team_name == "Robert Walraven")] <- FALSE
metadata$JHU_data[which(metadata$team_name == "COVID-19 Forecast Hub")] <- TRUE
metadata$ensemble <- ifelse(metadata$ensemble==TRUE,1,0)
metadata$compartmental <- ifelse(metadata$compartmental==TRUE,1,0)
metadata$stats <- ifelse(metadata$stats==TRUE,1,0)
# manual change 
#metadata <- mutate(filter(metadata, model_abbr == "Karlen-pypm"), model_type = "mechanistic")
# add text columns
metadata$model_type <- ifelse(metadata$ensemble, 
                                  "ensemble", 
                                  ifelse(metadata$stats + metadata$compartmental==2,
                                         "both stats and mech",
                                         ifelse((metadata$stats*2)+metadata$compartmental==2,
                                                "statistical",
                                                ifelse((metadata$stats*2)+metadata$compartmental==1,
                                                       "mechanistic",
                                                       "neither stats nor mech"))))
metadata$data_source <- ifelse(metadata$JHU_data,"JHU","unspecified")


# each target will have different sets of models based on the current filtering
## high count
wide_frame_hosp_high <- read.csv("./data/quantile_frame_hosp_top.csv") 

h_frame_high <- wide_frame_hosp_high %>% 
  frame_format2()

## low count
wide_frame_hosp_low <- read.csv("./data/quantile_frame_hosp_bottom.csv")

h_frame_low <- wide_frame_hosp_low %>% 
  frame_format2()

# model type
h_meta <- colnames(h_frame_high)[-c(1:6)] #note the same 5 models for high and low counts
```


```{r}
h_metadata <- metadata %>%
  dplyr::filter(model_abbr %in% h_meta)
recent_hmeta <- h_metadata %>%
  dplyr::group_by(team_name,model_name) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(model_abbr) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup()
recent_hmeta[5,24] <- "mechanistic" # change the Karlen model to mech
recent_hmeta[3,6] <- "public mobility data, CSSE, HHS, CDC scenario 5 data" 
short_hmeta <- recent_hmeta[,c(3,ncol(recent_hmeta)-1)]
short_hdata <- recent_hmeta[,c(3,ncol(recent_hmeta))]

# model inputs
short_hinput <- select(recent_hmeta, c(model_abbr, data_inputs))

# model methods
h_methods <- select(recent_hmeta, c(model_abbr, methods, methods_long)) 
# day of week effects won't be listed in methods
#   check for it by plotting the original forecast data to see trends w/in a single week

# calculate distance matrices
q_set_hosp <- unique(h_frame_high$quantile) 
approx_cd_list_high <- build_distance_frame(h_frame_high, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp,tau_G=q_set_hosp)

approx_cd_list2_high <- suppressWarnings(build_distance_frame(h_frame_high, 
                                        horizon_list=c(1:4),
                                        target_list="inc hosp",
                                        approx_rule="approximation2",
                                        tau_F=q_set_hosp,tau_G=q_set_hosp))

  
# extract data
total_frame_high <- approx_cd_list_high[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>% #mean cd across all locations
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_high$target_end_date <- as.Date(total_frame_high$target_end_date,origin="1970-01-01")

h_frame_mean_high <- lapply(1:4, function(x) cd_matrix(approx_cd_list_high[[3]],x))


# make data for box plot
newdf_high <- approx_cd_list_high[[3]][, c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_high[[3]])){
    newdf_high[i, ] = sort(newdf_high[i,c(1:3)])
}

pair_data_high <- approx_cd_list_high[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_high),] %>%
  dplyr::filter(model_1!=model_2)


# low count locations
approx_cd_list_low <- build_distance_frame(h_frame_low, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp,tau_G=q_set_hosp)

# approx_cd_list2_low <- build_distance_frame(d_frame_low, 
#                                         horizon_list=c(1:4),
#                                         target_list="inc death",
#                                         approx_rule="approximation2",
#                                         tau_F=q_set,tau_G=q_set)
# extract data
total_frame_low <- approx_cd_list_low[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_low$target_end_date <- as.Date(total_frame_low$target_end_date,origin="1970-01-01")

h_frame_mean_low <- lapply(1:4, function(x) cd_matrix(approx_cd_list_low[[3]],x))

newdf_low <- approx_cd_list_low[[3]][,c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_low[[3]])){
    newdf_low[i, ] = sort(newdf_low[i,c(1:3)])
}
pair_data_low <- approx_cd_list_low[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_low),] %>%
  dplyr::filter(model_1!=model_2)
```

There are 5 models that fulfilled the criteria for both the 5 locations with highest cumulative deaths and the 5 locations with lowest cumulative deaths.


### Model types

```{r}
knitr::kable(short_hmeta, col.names = c("Model","Type"))
```

### Differences between two approximations (for high count locations only)

The approximated pairwise Cramer's distances between each forecast and the ensemble are calculated using both types of approximations to check for any large discrepancies between the two methods. The table below shows the averaged approx. CD over all target end dates and all 5 high count locations.

```{r}
approx_cd_list_high[[3]] %>%
  dplyr::left_join(approx_cd_list2_high[[3]], by=c("horizon","model_1","model_2","target_variable")) %>%
  dplyr::filter(model_1=="COVIDhub-ensemble") %>%
  dplyr::mutate(diff=mean_dis.x-mean_dis.y) %>%
  dplyr::arrange(model_1,horizon,diff) %>%
  knitr::kable(.,col.names=c("Anchor Model", "Model","Horizon","Target",
                                     "CD (uneq)","CD (eq)","Diff"),digits=2,
               caption = "Mean approx. CDs relative to the ensemble")
```

We notice that Covid19Sim-Simulator has a much higher difference between its equal and unequal Cramer Distances compared to the other models, except for at a four week horizon, where JHUAPL-Bucky has a similarly high difference between its Cramer Distances.


We can visualize the mean approximated pairwise distances across all time points in a heat map shown below. The distance from the model to itself is zero. The $x-$axis is arranged based in an ascending order of the model's approximate pairwise distance from the COVIDhub-ensemble. So, the first model is the model that is most dissimilar (on average) to the ensemble in this time frame.

```{r,out.width="95%", fig.align='center', warning=FALSE}
distance_heatmap(approx_cd_list_high[[3]],
                 "Mean Approx. CD of Inc Hosp Forecasts by Horizon - High Hospitalization Count Locations",
                 recent_hmeta)
distance_heatmap(approx_cd_list_low[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - Low Hospitalization Count Locations",
                recent_hmeta)
```

It appears that the Covid19Sim-Simulator is the least similar to the other models for both the high count and low count locations, across all four horizons. JHUAPL-Bucky is the second least similar to the other models. However, high hospitalization count locations show a greater variation between models, especially at lower horizons. In low count locations, Covid19Sim-Simulator and JHUAPL-Bucky have more similar Cramer Distances. Of note is the greater difference between model at low horizons for high count locations, which is the opposite of the results shown for inc cases and inc deaths. 


We can also look at the approximated pairwise distances to see how the models become more similar or dissimilar over time.

```{r,out.width="95%", fig.align='center'}
ot_data_high <- total_frame_high %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup() 

ot_data_low <- total_frame_low %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup()

#new func throws error due to no metadata
scatter(ot_data_high,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nHigh Hospitalization Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
scatter(ot_data_low,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nLow Hospitalization Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
```

The scatterplots show that the Covid19Sim-Simulator and JHUAPL-Bucky models tend to differ from the Covidhub-ensemble model compared to the other two models. This seems to align with the results shown in the heat maps above that show that Covid19Sim-Simulator and JHUAPL-Bucky tend to have the highest mean Cramer's Distance from the other models. In high count locations, Covid19Sim-Simulator is very different from the ensemble model from January until April. However, in low count locations, JHUAPL-Bucky shows a peak in late March/early April, although this peak is not largely different, as the scale is pretty small. 

```{r,out.width="95%", fig.align='center', warning=FALSE, message=FALSE}
date_range <- seq.Date(from = as.Date("2020-12-17"), to = as.Date("2021-06-10"), by = "week")

hosp_truth_high <- load_truth("HealthData", 
                         "inc hosp", 
                         temporal_resolution="weekly",
                         data_location = "remote_hub_repo") %>%
  dplyr::filter(target_end_date >= min(unique(ot_data_high$target_end_date)),
                target_end_date <= max(unique(ot_data_high$target_end_date)),
                geo_type=="state",
                location_name %in% unique(wide_frame_hosp_high$location_name)) 



hosp_truth_low <- load_truth("HealthData", 
                         "inc hosp", 
                         temporal_resolution="weekly",
                         data_location = "remote_hub_repo") %>%
  dplyr::filter(target_end_date >= min(unique(ot_data_high$target_end_date)),
                target_end_date <= max(unique(ot_data_high$target_end_date)),
                geo_type=="state",
                location_name %in% unique(wide_frame_hosp_low$location_name)) 

# plot truth data
ggplot(hosp_truth_high, aes(x = target_end_date, y = value, color = location_name)) + 
      geom_point(alpha=0.6,size=0.8) + 
      stat_smooth(alpha=0.4,size=0.5,aes(x = target_end_date, y = value), method = "loess",
                  formula = y ~ x, se = FALSE) +
      ggtitle("Inc Hosp Over Time - High Count Locations") +
      ylab("Inc Hosp") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=9),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")

ggplot(hosp_truth_low, aes(x = target_end_date, y = value, color = location_name)) + 
      geom_point(alpha=0.6,size=0.8) + 
      stat_smooth(alpha=0.4,size=0.5,aes(x = target_end_date, y = value), method = "loess",
                  formula = y ~ x, se = FALSE) +
      ggtitle("Inc Hosp Over Time - Low Count Locations") +
      ylab("Inc Hosp") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=9),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")
```
It seems that the Covid19Sim-Simulator's differences from the ensemble model are follow the trends shown by the truth data. 


```{r}
hosp_truth_us <- load_truth("HealthData", 
                         "inc hosp", 
                         temporal_resolution="weekly",
                         data_location = "remote_hub_repo") %>%
  dplyr::filter(target_end_date >= min(unique(ot_data_high$target_end_date)),
                target_end_date <= max(unique(ot_data_high$target_end_date)),
                geo_type=="state",
                location == "US") 

h_forecast <- load_latest_forecasts(
  locations = "US",
  targets = paste(1:31, "day ahead inc hosp"),
  last_forecast_date = as.Date("2021-01-01") ,
  forecast_date_window_size = 6,
  source = "zoltar")


plot1 <- plot_forecasts(filter(h_forecast, (model %in% h_meta)), 
                     target_variable = "inc hosp", 
                     truth_source="HealthData", 
                     subtitle = "none", 
                     title = "none",
                     show_caption=FALSE,
                     fill_by_model = TRUE, fill_transparency = .3,
                     intervals=NULL) + 
  scale_x_date(name=NULL, date_breaks = "1 month", date_labels = "%b '%y", limits=c(as.Date(NA), forecast_date+32), expand = expansion(mult=c(0.02,0.02))) + 
  theme(axis.ticks.length.x = unit(0.5, "cm"),  
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        plot.title = element_blank())

plot1  

#plot1_1 <- 
  plot1 +
  scale_x_date(name=NULL, 
               date_breaks = "1 month", 
               date_labels = "%b '%y", 
               limits = c(as.Date("2021-01-01"), as.Date("2021-01-31"))) 
```

The truth data has a cyclic pattern that indicates that there may be a day of the week effect such that incident hospitalizations increase during the week but fall on the weekend. 

It seems that both Covid19Sim-Simulator and JHUAPL-Bucky predict similar trends for January 2021, except Covid19Sim-Simulator has a higher "intercept", which leads to its Cramer's Distance between it and the COVIDhub-ensemble much greater than that of JHUAPL-Bucky. 


We can also cluster the distances using hierarchical clustering. 

```{r,out.width="95%", fig.align='center', message=FALSE, warning=FALSE}
for(i in 1:4){
    assign(paste0("dp_",i),
           dendro_plot(i, "h_frame_mean_high",short_hmeta)
             )
    assign(paste0("dpd_",i),
           dendro_plot(i, "h_frame_mean_high",short_hdata,FALSE)
             )
    assign(paste0("dpl_",i),
           dendro_plot(i, "h_frame_mean_low",short_hmeta)
             )
    assign(paste0("dpdl_",i),
           dendro_plot(i, "h_frame_mean_low",short_hdata,FALSE)
             )
  } 
grid.arrange(dp_1,dp_2,dp_3,dp_4,nrow=2)
```



```{r,fig.align='center',fig.cap="Low Mortality Count Locations",fig.height=4}
grid.arrange(dpl_1,dpl_2,dpl_3,dpl_4,nrow=2)
```

For both high count and low count locations, all of the dendrograms show that Covid19Sim-Simulator is the most dissimilar to the other models, followed by JHUAPL-Bucky. However, the scale for differences in Cramer Distance for the low count locations is very small, which may be a result of low hospitalizations.

   

Overall, it seems that Covid19Sim-Simulator and JHUAPL-Bucky are pretty consistently the two models then are the most dissimilar from the other three models, across almost all horizons for both high-count and low count regions. 

