---
title: "COVID-19 Forecast Similarity Analysis for Hospitalizations"
author: "Johannes Bracher, Evan Ray, Nick Reich, Nutcha Wattanachit, Li Shandross"
date: "07/20/2021"
header-includes:
   - \usepackage{tabularx}
   - \usepackage{hyperref}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{xcolor}
output:
  pdf_document:
        latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(energy)
library(knitr)
library(data.table)
library(covidHubUtils)
#devtools::install_github("reichlab/covidHubUtils")
library(lubridate)
library(zoltr)
library(igraph)
library(gtools)
library(gridExtra)
library(ggdendro)
library(anytime) # note that the Rcpp must be loaded to use the anytime package
library(Rcpp)
library(patchwork)
knitr::opts_chunk$set(echo=FALSE,
                       comment = FALSE, message=FALSE, fig.show= 'hold',fig.pos="H",table.placement='H',
                       fig.align = 'center')
```


# COVID-19 Forecasting Model Similarity Analysis for 1-4 Week Ahead Incident Hospitalization 

In this analysis, we extend the work of Bracher, et. al that evaluates the similarity between Covid-19 models using Cramer's distance. This work was applied to forecasts for incident deaths (inc deaths) and incident cases (inc cases) and here we apply it to incident hospitalizations (inc hosp).

We must first make some adjustments to the inc hosp forecast data, which have a temporal resolution of "day" instead of "week," unlike inc case and inc death. This presents a challenge because the horizons will be different for the same target end date if the forecast dates between two models differ by only a single day. (This is not an issue when the temporal resolution is in terms of weeks, which are defined by epidemiological week, not the number of days between forecast date and target end date.) Thus, we create a new variable called horizon week to solve this issue. This variable counts horizons between 1 and 7 to have a horizon week of 1, horizons between 8 and 14 to have a horizon week of 2, etc. Hence, the analyses used on inc death and inc cases should be able to be applied easily to the inc hosp data.

The pairwise approximated Cramer's distances are calculated for the models that have complete submissions for all targets, all probability levels, and no missing forecasts between January 28th, 2020 and June 10th 2021. We aggregate results for the five locations that have the highest number of number of COVID-19 hospitalizations during this period as well as the five locations with the lowest number for the same date range.

```{r forecasts and functions}
source("../functions/distance_func_script.R")
source("../functions/distance_func_hosp_script.R")

# set targets for analysis
target_horizon <- 1:4
target_var <- "inc hosp"


# read in forecasts
## high count - Thursday
wide_frame_hosp_high_thurs <- read.csv("../data/quantile_frame_hosp_top.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_hosp_high_thurs$target_end_date <- anydate(wide_frame_hosp_high_thurs$target_end_date)

h_frame_high_thurs <- wide_frame_hosp_high_thurs %>% 
  frame_format2()

## low count - Thursday
wide_frame_hosp_low_thurs <- read.csv("../data/quantile_frame_hosp_bottom.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_hosp_low_thurs$target_end_date <- anydate(wide_frame_hosp_low_thurs$target_end_date)

h_frame_low_thurs <- wide_frame_hosp_low_thurs %>% 
  frame_format2()


## high count - Saturday
wide_frame_hosp_high_sat <- read.csv("../data/quantile_frame_hosp_top_sat.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_hosp_high_sat$target_end_date <- anydate(wide_frame_hosp_high_sat$target_end_date)

h_frame_high_sat <- wide_frame_hosp_high_sat %>% 
  frame_format2() %>%
  select(-c("LANL-GrowthRate")) # LANL doesn't meet criteria for Thursdays

## low count - Saturday
wide_frame_hosp_low_sat <- read.csv("../data/quantile_frame_hosp_bottom_sat.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_hosp_low_sat$target_end_date <- anydate(wide_frame_hosp_low_sat$target_end_date)

h_frame_low_sat <- wide_frame_hosp_low_sat %>% 
  frame_format2() %>%
  select(-c("LANL-GrowthRate")) # LANL doesn't meet criteria for Thursdays



## truth data
filtered_start_date <- as.Date("2021-02-18")
end_date_sat <- as.Date("2021-06-12")

hosp_truth_high <- load_truth("HealthData", 
                         "inc hosp", 
                         temporal_resolution="weekly",
                         data_location = "remote_hub_repo")
hosp_truth_high <- hosp_truth_high %>%
  dplyr::filter(target_end_date >= filtered_start_date,
                target_end_date <= end_date_sat,
                geo_type=="state",
                location_name %in% unique(wide_frame_hosp_high_thurs$location_name)) 

hosp_truth_low <- load_truth("HealthData", 
                         "inc hosp", 
                         temporal_resolution="weekly",
                         data_location = "remote_hub_repo") %>%
  dplyr::filter(target_end_date >= filtered_start_date,
                target_end_date <= end_date_sat,
                geo_type=="state",
                location_name %in% unique(wide_frame_hosp_low_thurs$location_name)) 
```

There are nine models that fulfilled the criteria for both the five high count and five low count locations for Thursday forecasts. There are ten that fulfill such criteria for the Saturday forecasts, but we perform the analysis on only the overlapping nine models.


```{r metadata, warning=FALSE, message=FALSE}
# read in model metadata
metadata <- read.csv("../metadata_categorized.csv") 
metadata$stats[which(metadata$team_name == "Karlen Working Group")] <- TRUE
metadata$compartmental[which(metadata$team_name == "Robert Walraven")] <- FALSE
metadata$JHU_data[which(metadata$team_name == "COVID-19 Forecast Hub")] <- TRUE
metadata$ensemble <- ifelse(metadata$ensemble==TRUE,1,0)
metadata$compartmental <- ifelse(metadata$compartmental==TRUE,1,0)
metadata$stats <- ifelse(metadata$stats==TRUE,1,0)
# manual change 
# add text columns
metadata$model_type <- ifelse(metadata$ensemble, 
                                  "ensemble", 
                                  ifelse(metadata$stats + metadata$compartmental==2,
                                         "both stats and mech",
                                         ifelse((metadata$stats*2)+metadata$compartmental==2,
                                                "statistical",
                                                ifelse((metadata$stats*2)+metadata$compartmental==1,
                                                       "mechanistic",
                                                       "neither stats nor mech"))))
metadata$data_source <- ifelse(metadata$JHU_data,"JHU","unspecified")

h_meta <- colnames(h_frame_high_thurs)[-c(1:6)] #note the same 9 models for high and low counts

h_metadata <- metadata %>%
  dplyr::filter(model_abbr %in% h_meta)
recent_hmeta <- h_metadata %>%
  dplyr::group_by(team_name,model_name) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(model_abbr) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup()
recent_hmeta[8,24] <- "mechanistic" # change the Karlen model to mech
recent_hmeta[5,6] <- "public mobility data, CSSE, HHS, CDC scenario 5 data" 
short_hmeta <- recent_hmeta[,c(3,ncol(recent_hmeta)-1)]
short_hdata <- recent_hmeta[,c(3,ncol(recent_hmeta))]

# model inputs
short_hinput <- select(recent_hmeta, c(model_abbr, data_inputs))

# model methods
h_methods <- select(recent_hmeta, c(model_abbr, methods, methods_long)) 

```


### Day of the Week Effects

We can plot point forecasts to see if any of the models incorporate day of the week effects (i.e. do forecasts certain days of the week specifically show lower/higher hospitalizations compared to other days). 

```{r, out.width="95%", fig.align='center', message=FALSE, warning=FALSE, comment=FALSE}
###### Plot model forecasts and truth data for high hosp locs -------------------------------------- #
# truth data
hosp_truth_high2 <- hosp_truth_high %>% 
  select(-c(location, 6:11)) %>%
  group_by(target_end_date) %>%
  mutate(agg_hosp = sum(value)) %>%
  distinct(agg_hosp, .keep_all = TRUE) %>%
  mutate(day_type = ifelse(wday(target_end_date) %in% 2:6, "weekday", "weekend"))

ggplot(hosp_truth_high2, aes(group = day_type)) + 
  geom_point(aes(x = target_end_date, y = agg_hosp, color = day_type)) +
  geom_line(aes(x = target_end_date, y = agg_hosp, linetype = day_type, alpha = 0.5)) +
  labs(title = "Truth Data") +
  scale_color_manual(values = c("#dd7e6b", "#1c4587")) + 
  theme(legend.position = "none")

# forecasts
hosp_pt_high <- read.csv("../data/point_frame_hosp_top.csv")

hosp_pt_high$forecast_date <- anydate(hosp_pt_high$forecast_date)

hosp_pt_high <- hosp_pt_high %>%
  select(-c(quantile, location, 11:16)) %>%
  group_by(model, target_end_date, horizon_week, horizon) %>%
  mutate(agg_hosp = sum(value)) %>%
  distinct(agg_hosp, .keep_all = TRUE) %>%
  filter(horizon_week == 1) %>%
  mutate(day_type = ifelse(wday(target_end_date) %in% 2:6, "weekday", "weekend")) %>%
  mutate(forecast_day = weekdays(forecast_date))

hosp_pt_high$target_end_date <- anydate(hosp_pt_high$target_end_date)
hosp_pt_high$day_type <- as.factor(hosp_pt_high$day_type)

plot_day_of_week_effect_sep(hosp_pt_high, "COVIDhub-ensemble") # as.Date("2021-02-13")
plot_day_of_week_effect_sep(hosp_pt_high, "JHUAPL-Bucky") # as.Date("2021-02-27")
plot_day_of_week_effect_sep(hosp_pt_high, "JHUAPL-Gecko")
plot_day_of_week_effect_sep(hosp_pt_high, "Google_Harvard-CPF") 

plot_day_of_week_effect_sep(hosp_pt_high, "Covid19Sim-Simulator")
plot_day_of_week_effect_sep(hosp_pt_high, "CU-select")
plot_day_of_week_effect_sep(hosp_pt_high, "Karlen-pypm") 
plot_day_of_week_effect_sep(hosp_pt_high, "JHUAPL-SLPHospEns")
plot_day_of_week_effect_sep(hosp_pt_high, "MOBS-GLEAM_COVID")
```

Truth data has always shown day of the week effects (weekends always have lower inc hosp) but the models seemed to predict lower inc hosp on Tuesdays for the end of December/beginning of January, then most seem to switch to lower weekend end inc hosp sometime during 2021. 

```{r,out.width="95%", fig.align='center', message=FALSE, warning=FALSE}
###### Plot model forecasts and truth data for low hosp locs ---------------------------------------- #

# truth data
hosp_truth_low2 <- hosp_truth_low %>% 
 # select(-c(location, 6:11)) %>%
  group_by(target_end_date) %>%
  mutate(agg_hosp = sum(value)) %>%
  distinct(agg_hosp, .keep_all = TRUE) %>%
  mutate(day_type = ifelse(wday(target_end_date) %in% 2:6, "weekday", "weekend"))

ggplot(hosp_truth_low2, aes(group = day_type)) + 
  geom_point(aes(x = target_end_date, y = agg_hosp, color = day_type)) +
  geom_line(aes(x = target_end_date, y = agg_hosp, linetype = day_type)) +
  labs(title = "Truth Data")

# foreasts
hosp_pt_low <- read.csv("../data/point_frame_hosp_bottom.csv")

hosp_pt_low <- hosp_pt_low %>%
  select(-c(quantile, location, 11:16)) %>%
  group_by(model, target_end_date, horizon_week) %>%
  mutate(agg_hosp = sum(value)) %>%
  distinct(agg_hosp, .keep_all = TRUE) %>%
  filter(horizon_week == 1) %>%
  mutate(day_type = ifelse(wday(target_end_date) %in% 2:6, "weekday", "weekend"))

hosp_pt_low$target_end_date <- anydate(hosp_pt_low$target_end_date)
hosp_pt_low$day_type <- as.factor(hosp_pt_low$day_type)

plot_day_of_week_effect_sep(hosp_pt_low, "COVIDhub-ensemble") # as.Date("2021-02-13")
plot_day_of_week_effect_sep(hosp_pt_low, "JHUAPL-Bucky") # as.Date("2021-02-27")
plot_day_of_week_effect_sep(hosp_pt_low, "JHUAPL-Gecko")
plot_day_of_week_effect_sep(hosp_pt_low, "Google_Harvard-CPF") 

plot_day_of_week_effect_sep(hosp_pt_low, "Covid19Sim-Simulator")
plot_day_of_week_effect_sep(hosp_pt_low, "CU-select")
plot_day_of_week_effect_sep(hosp_pt_low, "Karlen-pypm") 
plot_day_of_week_effect_sep(hosp_pt_low, "JHUAPL-SLPHospEns")
plot_day_of_week_effect_sep(hosp_pt_low, "MOBS-GLEAM_COVID")
```

The low hosp location truth data doesn't present such a clear pattern as to day of the week effects that suggest that weekends or weekdays generally have less inc hosp values. However, most of the models seem to switch to incorporating a day of the week effect at about the same time as that for the high count values. 

```{r}
recent_hmeta$day_of_wk_effect <- 
  as.factor(c("FALSE", "ENS incl", "FALSE", "FALSE", "TRUE", "TRUE", "ENS excl", "FALSE", "FALSE"))

short_hday <- select(recent_hmeta, c(model_abbr, day_of_wk_effect))

knitr::kable(short_hday, col.names = c("Model","Day of Week Effect"))

metadata <- metadata %>%
  left_join(select(recent_hmeta, c(model_name, day_of_wk_effect)), by = c("model_name"))

day_test <- hosp_pt_high %>%
  group_by(model) %>%
  distinct(forecast_day) %>%
  filter(model %in% short_hmeta$model_abbr) 
  # mobs was on sun instead of monday for forecasts made on 5/02 
  # gecko was Sun on 1/24, 3/14, 4/04 - 6/06 ; Mon on 2/01 - 3/08, 3/22, 3/29
  # google was Mon 1/25, 2/01, 2/15, 2/22, 3/08, 3/29 - 5/10, 5/24; 
  #            Sun 2/07, 2/28, 3/14, 3/21, 5/16, 5/30, 6/06
  #
  # since all incl. models forecast on Sun or Mon, we generally don't have to worry re
  # any one having an unfair advantage over another when it comes to forecasting during
  # the same horizon week
```


## Weekday Analysis
This analysis only examines target end dates for a single day of the week, Thursday, to account for models that include day of the week effects. 

```{r build data and figures}
# calculate distance matrices
q_set_hosp_thurs <- unique(h_frame_high_thurs$quantile) 
approx_cd_list_high_thurs <- build_distance_frame(h_frame_high_thurs, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp_thurs,tau_G=q_set_hosp_thurs)

approx_cd_list2_high <- suppressWarnings(build_distance_frame(h_frame_high_thurs, 
                                        horizon_list=c(1:4),
                                        target_list="inc hosp",
                                        approx_rule="approximation2",
                                        tau_F=q_set_hosp_thurs,tau_G=q_set_hosp_thurs))

  
# extract data
total_frame_high_thurs <- approx_cd_list_high_thurs[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>% #mean cd across all locations
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_high_thurs$target_end_date <- as.Date(total_frame_high_thurs$target_end_date,origin="1980-01-01")

h_frame_mean_high_thurs <- lapply(1:4, function(x) cd_matrix(approx_cd_list_high_thurs[[3]],x))


# make data for box plot
newdf_high <- approx_cd_list_high_thurs[[3]][, c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_high_thurs[[3]])){
    newdf_high[i, ] = sort(newdf_high[i,c(1:3)])
}

pair_data_high <- approx_cd_list_high_thurs[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_high),] %>%
  dplyr::filter(model_1!=model_2)


# low count locations
approx_cd_list_low_thurs <- build_distance_frame(h_frame_low_thurs, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp_thurs,tau_G=q_set_hosp_thurs)

# approx_cd_list2_low <- build_distance_frame(d_frame_low, 
#                                         horizon_list=c(1:4),
#                                         target_list="inc death",
#                                         approx_rule="approximation2",
#                                         tau_F=q_set,tau_G=q_set)
# extract data
total_frame_low_thurs <- approx_cd_list_low_thurs[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_low_thurs$target_end_date <- as.Date(total_frame_low_thurs$target_end_date,origin="1970-01-01")

h_frame_mean_low_thurs <- lapply(1:4, function(x) cd_matrix(approx_cd_list_low_thurs[[3]],x))

newdf_low <- approx_cd_list_low_thurs[[3]][,c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_low_thurs[[3]])){
    newdf_low[i, ] = sort(newdf_low[i,c(1:3)])
}
pair_data_low <- approx_cd_list_low_thurs[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_low),] %>%
  dplyr::filter(model_1!=model_2)
```


We can visualize the mean approximated pairwise distances across all time points in a heat map shown below. The distance from the model to itself is zero. The $x-$axis is arranged based in an ascending order of the model's approximate pairwise distance from the COVIDhub-ensemble. So, the first model is the model that is most dissimilar (on average) to the ensemble in this time frame.


```{r,out.width="95%", fig.align='center', warning=FALSE}
distance_heatmap_wk(approx_cd_list_high_thurs[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - High Count Locations", recent_hmeta)  

distance_heatmap_wk(approx_cd_list_low_thurs[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - Low Count Locations", recent_hmeta)
```

Google_Harvard-CPF is generally the least similar to the other models for both the high count and low count locations. This is true across all horizons for the high count locations, but only true for the one and two week horizons of the low count locations. Covid19Sim-Simulator, JHUAPL-Bucky, and JHUAPL-Gecko show similar Cramer's Distances at three and four week horizons at the low count locations. However, it is important to note the small scale observed for the low count locations may explain why three models have such similar Cramer's Distances.

Covid19Sim-Simulator is the second least similar model for the high count locations, but its Cramer's Distance is much smaller than that of Google_Harvard-CPF. Of note is similar differences between models at all horizons for high count locations, unlike the results shown for inc cases and inc deaths which show substantial differences between models as horizon length increases. 

For low count locations, models with day of the week effect tend to have higher Cramer's Distances, but there doesn't seem to be much of a pattern for high count locations. 


We can also look at the approximated pairwise distances to see how the models become more similar or dissimilar over time.




```{r,out.width="95%", fig.align='center'}
ot_data_high_thurs <- total_frame_high_thurs %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup() 

ot_data_low_thurs <- total_frame_low_thurs %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup()


scatter_wk(ot_data_high_thurs,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nHigh Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
scatter_wk(ot_data_low_thurs,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nLow Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
```

The scatterplots show that the Google_Harvard-CPF, Covid19Sim-Simulator, and JHUAPL-Bucky models tend to differ from the Covidhub-ensemble model compared to the other models. This seems to align with the results shown in the heat maps above that show that Google_Harvard-CPF, Covid19Sim-Simulator, and JHUAPL-Bucky tend to have the highest mean Cramer's Distance from the other models. In high count locations, Google_Harvard-CPF is very different from the ensemble model from February until April. However, in low count locations, JHUAPL-Bucky shows a peak in around March, although this peak is not largely different, as the scale is pretty small.

Whether models incorporate a day of the week effect does not seem to have an impact on how much the model differs from the ensemble, nor as to when it differs greatly. 


```{r,out.width="95%", fig.align='center', warning=FALSE, message=FALSE}
# plot truth data
ggplot(hosp_truth_high, aes(x = target_end_date, y = value, color = location_name)) + 
      geom_point(alpha=0.6,size=0.8) + 
      stat_smooth(alpha=0.4,size=0.5,aes(x = target_end_date, y = value), method = "loess",
                  formula = y ~ x, se = FALSE) +
      ggtitle("Hosp Truth Over Time - High Count Locations") +
      ylab("Inc Hosp") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=9),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")

ggplot(hosp_truth_low, aes(x = target_end_date, y = value, color = location_name)) + 
      geom_point(alpha=0.6,size=0.8) + 
      stat_smooth(alpha=0.4,size=0.5,aes(x = target_end_date, y = value), method = "loess",
                  formula = y ~ x, se = FALSE) +
      ggtitle("Hosp Truth Over Time - Low Count Locations") +
      ylab("Inc Hosp") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=9),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")
```
It seems that Google_Harvard-CPF and Covid19Sim-Simulator's differences from the ensemble model follow the trends shown by the truth data. 


```{r,out.width="95%", fig.align='center', message=FALSE, warning=FALSE}
cd_diff_high_thurs <- total_frame_high_thurs %>%
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="Google_Harvard-CPF",
                model_2 == "Covid19Sim-Simulator") %>%
  dplyr:: ungroup() 

cd_diff_high_thurs$target_end_date <- anydate(cd_diff_high_thurs$target_end_date)

ggplot(cd_diff_high_thurs, aes(x=target_end_date, y=mean_approx_cd)) + 
   #   geom_point(alpha=0.6,size=0.8) + 
      geom_line(alpha=0.4) +
      ggtitle("Approx CD over Time - Covid19Sim-Simulator and Google_Harvard-CPF") +
      ylab("Approx. CD") +
      xlab("Forecast End Date") +
      facet_wrap(vars(horizon), nrow = 2,scales = "free") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=7),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")


cd_diff_low_thurs <- total_frame_low_thurs %>%
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="Google_Harvard-CPF",
                model_2 == "Covid19Sim-Simulator") %>%
  dplyr:: ungroup() 

cd_diff_low_thurs$target_end_date <- anydate(cd_diff_low_thurs$target_end_date)

ggplot(cd_diff_low_thurs, aes(x=target_end_date, y=mean_approx_cd)) + 
   #   geom_point(alpha=0.6,size=0.8) + 
      geom_line(alpha=0.4) +
      ggtitle("Approx CD over Time - Covid19Sim-Simulator and Google_Harvard-CPF") +
      ylab("Approx. CD") +
      xlab("Forecast End Date") +
      facet_wrap(vars(horizon), nrow = 2,scales = "free") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=7),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")
```
These plots indicate that the difference between the models at the high count locations seems to shrink over time. Since the low count locations approx CD is so low, it is difficult to draw conclusions from the plots. 



```{r,out.width="95%", fig.align='center', message=FALSE, warning=FALSE}
hosp_truth_ca_ny <- hosp_truth_high %>%
  filter(location %in% c("06", "36"))
  
hosp_pt_ca_ny <- read.csv("../data/point_frame_hosp_top.csv")  

hosp_pt_ca_ny <- hosp_pt_ca_ny %>% 
  select(-c(quantile, forecast_date, type, temporal_resolution)) %>%
  dplyr::mutate(horizon_week = case_when(
    horizon %in% 1:7 ~ 1,
    horizon %in% 8:14 ~ 2, 
    horizon %in% 15:21 ~ 3,
    horizon %in% 21:28 ~ 4,
  )) %>%
  filter(location %in% c("6", "36")) %>%
  filter(target_end_date >= filtered_start_date, 
         target_end_date <= max(unique(h_frame_high_thurs$target_end_date)))  %>%
  filter(model %in% c("JHUAPL-SLPHospEns", "COVIDhub-ensemble")) %>%
  filter(horizon_week == 1) %>%
  select(-c(horizon_week, horizon)) %>%
  select(c(colnames(hosp_truth_ca_ny)))


hosp_truth_forecast_ca_ny <- rbind(hosp_truth_ca_ny, hosp_pt_ca_ny)
hosp_truth_forecast_ca_ny$target_end_date <- anydate(hosp_truth_forecast_ca_ny$target_end_date)

# plot truth data
ggplot(hosp_truth_forecast_ca_ny, 
       aes(x = target_end_date, y = value, color = model, shape = location_name)) + 
      geom_point(alpha=0.6) + 
      geom_line() +
      ggtitle("Hosp Truth and Forecasts Over Time") +
      ylab("Inc Hosp") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=9),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")
```


We can also cluster the distances using hierarchical clustering. 

```{r,out.width="95%", fig.cap="High Hospitalization Count Locations", fig.align='center', message=FALSE, warning=FALSE}
for(i in 1:4){
    assign(paste0("dp_t",i),
           dendro_plot_wk(i, "h_frame_mean_high_thurs",short_hday)
             )
    assign(paste0("dpl_t",i),
           dendro_plot_wk(i, "h_frame_mean_low_thurs",short_hday)
             )
}

grid.arrange(dp_t1,dp_t2,dp_t3,dp_t4)
```


```{r,out.width="95%", fig.align='center',fig.cap="Low Hospitalization Count Locations",fig.height=4}
grid.arrange(dpl_t1,dpl_t2,dpl_t3,dpl_t4,nrow=2)
```

For both high count and low count locations, all of the dendrograms show that Google_Harvard-CPF is generally the most dissimilar to the other models, and substantially so for the high count locations, followed by Covid19Sim-Simulator. However, the scale for differences in Cramer Distance for the low count locations is very small, which may be a result of low hospitalizations, which may explain why Google_Harvard-CPF and Covid19Sim-Simulator show similar Cramer's Distances for low count locations. We can also see that the ensembles are the most similar among models.


Overall, it seems that Google_Harvard-CPF is consistently the most dissimilar from the other models by a substantial amount, followed by Covid19Sim-Simulator, across almost all horizons for both high-count and low count regions. For Thursday forecasts, it seems models without day of the week effects tend to be more dissimilar from the COVIDhub-ensemble model than models without day of the week effects. 


## Weekend Analysis

Now we look at forecasts with a target end date on a Saturday to see if day of the week effects change model similarity. We expect forecasts for weekends to show the impact of day of the week effects more strongly than weekdays. 

```{r build data and figures}
# calculate distance matrices
q_set_hosp_sat <- unique(h_frame_high_sat$quantile) 
approx_cd_list_high_sat <- build_distance_frame(h_frame_high_sat, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp_sat,tau_G=q_set_hosp_sat)

approx_cd_list2_high <- suppressWarnings(build_distance_frame(h_frame_high_sat, 
                                        horizon_list=c(1:4),
                                        target_list="inc hosp",
                                        approx_rule="approximation2",
                                        tau_F=q_set_hosp_sat,tau_G=q_set_hosp_sat))

  
# extract data
total_frame_high_sat <- approx_cd_list_high_sat[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>% #mean cd across all locations
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_high_sat$target_end_date <- as.Date(total_frame_high_sat$target_end_date,origin="1980-01-01")

h_frame_mean_high_sat <- lapply(1:4, function(x) cd_matrix(approx_cd_list_high_sat[[3]],x))


# make data for box plot
newdf_high <- approx_cd_list_high_sat[[3]][, c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_high_sat[[3]])){
    newdf_high[i, ] = sort(newdf_high[i,c(1:3)])
}

pair_data_high <- approx_cd_list_high_sat[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_high),] %>%
  dplyr::filter(model_1!=model_2)


# low count locations
approx_cd_list_low_sat <- build_distance_frame(h_frame_low_sat, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp_sat,tau_G=q_set_hosp_sat)

# approx_cd_list2_low <- build_distance_frame(d_frame_low, 
#                                         horizon_list=c(1:4),
#                                         target_list="inc death",
#                                         approx_rule="approximation2",
#                                         tau_F=q_set,tau_G=q_set)
# extract data
total_frame_low_sat <- approx_cd_list_low_sat[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_low_sat$target_end_date <- as.Date(total_frame_low_sat$target_end_date,origin="1970-01-01")

h_frame_mean_low_sat <- lapply(1:4, function(x) cd_matrix(approx_cd_list_low_sat[[3]],x))

newdf_low <- approx_cd_list_low_sat[[3]][,c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_low_sat[[3]])){
    newdf_low[i, ] = sort(newdf_low[i,c(1:3)])
}
pair_data_low <- approx_cd_list_low_sat[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_low),] %>%
  dplyr::filter(model_1!=model_2)
```


We can visualize the mean approximated pairwise distances across all time points in a heat map shown below. The distance from the model to itself is zero. The $x-$axis is arranged based in an ascending order of the model's approximate pairwise distance from the COVIDhub-ensemble. So, the first model is the model that is most dissimilar (on average) to the ensemble in this time frame.

```{r,out.width="95%", fig.align='center', warning=FALSE}
distance_heatmap_wk(approx_cd_list_high_sat[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - High Count Locations", recent_hmeta)  

distance_heatmap_wk(approx_cd_list_low_sat[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - Low Count Locations", recent_hmeta)
```

Similarly to the Thursday forecasts, Google_Harvard-CPF is generally the least similar to the other models for both the high count and low count locations. This is true across all horizons for the high count locations, but only true for the one and two week horizons of the low count locations. Covid19Sim-Simulator is the most dissimilar for the three and four week horizons of the low count locations. However, it is important to note the small scale observed for the low count locations may explain why this model has a higher Cramer's Distance at longer horizons rather than a true pattern.

Covid19Sim-Simulator is the second least similar model for the high count locations, but its Cramer's Distance is much smaller than that of Google_Harvard-CPF. Of note is similar differences between models at all horizons for high count locations, unlike the results shown for inc cases and inc deaths which show substantial differences between models as horizon length increases. 

For high count locations, models without day of the week effect tend to have higher Cramer's Distances, but it is unclear if there is a pattern for low count locations. 

The Saturday forecasts show similar but slightly different heat maps to the Thursday forecasts.


We can also look at the approximated pairwise distances to see how the models become more similar or dissimilar over time.

```{r,out.width="95%", fig.align='center'}
ot_data_high_sat <- total_frame_high_sat %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup() 

ot_data_low_sat <- total_frame_low_sat %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup()


scatter_wk(ot_data_high_sat,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nHigh Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
scatter_wk(ot_data_low_sat,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nLow Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
```

The scatterplots show that the Google_Harvard-CPF, Covid19Sim-Simulator, and JHUAPL-Bucky models tend to differ from the Covidhub-ensemble model compared to the other models. This seems to align with the results shown in the heat maps above that show that Google_Harvard-CPF, Covid19Sim-Simulator, and JHUAPL-Bucky tend to have the highest mean Cramer's Distance from the other models. In high count locations, Google_Harvard-CPF is very different from the ensemble model from February until April. However, in low count locations, JHUAPL-Bucky shows a peak in around March, although this peak is not largely different, as the scale is pretty small.

Whether models incorporate a day of the week effect does not seem to have an impact on how much the model differs from the ensemble, nor as to when it differs greatly. 

These scatterplots are nearly the same as the ones shown above. 


```{r,out.width="95%", fig.align='center', warning=FALSE, message=FALSE}
# plot truth data - note these are repeated figures from above
ggplot(hosp_truth_high, aes(x = target_end_date, y = value, color = location_name)) + 
      geom_point(alpha=0.6,size=0.8) + 
      stat_smooth(alpha=0.4,size=0.5,aes(x = target_end_date, y = value), method = "loess",
                  formula = y ~ x, se = FALSE) +
      ggtitle("Hosp Truth Over Time - High Count Locations") +
      ylab("Inc Hosp") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=9),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")

ggplot(hosp_truth_low, aes(x = target_end_date, y = value, color = location_name)) + 
      geom_point(alpha=0.6,size=0.8) + 
      stat_smooth(alpha=0.4,size=0.5,aes(x = target_end_date, y = value), method = "loess",
                  formula = y ~ x, se = FALSE) +
      ggtitle("Hosp Truth Over Time - Low Count Locations") +
      ylab("Inc Hosp") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=9),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")
```
Like with the Thursday forecasts, it seems that Google_Harvard-CPF and Covid19Sim-Simulator's differences from the ensemble model follow the trends shown by the truth data. 


```{r,out.width="95%", fig.align='center', message=FALSE, warning=FALSE}
cd_diff_high_sat <- total_frame_high_sat %>%
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="Google_Harvard-CPF",
                model_2 == "Covid19Sim-Simulator") %>%
  dplyr:: ungroup() 

cd_diff_high_sat$target_end_date <- anydate(cd_diff_high_sat$target_end_date)

ggplot(cd_diff_high_sat, aes(x=target_end_date, y=mean_approx_cd)) + 
   #   geom_point(alpha=0.6,size=0.8) + 
      geom_line(alpha=0.4) +
      ggtitle("Approx CD over Time - Covid19Sim-Simulator and Google_Harvard-CPF") +
      ylab("Approx. CD") +
      xlab("Forecast End Date") +
      facet_wrap(vars(horizon), nrow = 2,scales = "free") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=7),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")


cd_diff_low_sat <- total_frame_low_sat %>%
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="Google_Harvard-CPF",
                model_2 == "Covid19Sim-Simulator") %>%
  dplyr:: ungroup() 

cd_diff_low_sat$target_end_date <- anydate(cd_diff_low_sat$target_end_date)

ggplot(cd_diff_low_sat, aes(x=target_end_date, y=mean_approx_cd)) + 
   #   geom_point(alpha=0.6,size=0.8) + 
      geom_line(alpha=0.4) +
      ggtitle("Approx CD over Time - Covid19Sim-Simulator and Google_Harvard-CPF") +
      ylab("Approx. CD") +
      xlab("Forecast End Date") +
      facet_wrap(vars(horizon), nrow = 2,scales = "free") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=7),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'))+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")
```
Like with the Thursday forecasts, these plots indicate that the difference between the models at the high count locations seems to shrink over time. Since the low count locations approx CD is so low, it is difficult to draw conclusions from the plots. 


We can also cluster the distances using hierarchical clustering. 

```{r,out.width="95%", fig.cap="High Hospitalization Count Locations", fig.align='center', message=FALSE, warning=FALSE}
for(i in 1:4){
    assign(paste0("dp_s",i),
           dendro_plot_wk(i, "h_frame_mean_high_sat",short_hday)
             )
    assign(paste0("dpl_s",i),
           dendro_plot_wk(i, "h_frame_mean_low_sat",short_hday)
             )
}

grid.arrange(dp_s1,dp_s2,dp_s3,dp_s4)
```


```{r,out.width="95%", fig.align='center',fig.cap="Low Hospitalization Count Locations",fig.height=4}
grid.arrange(dpl_s1,dpl_s2,dpl_s3,dpl_s4,nrow=2)
```

For both high count and low count locations, all of the dendrograms show that Google_Harvard-CPF is generally the most dissimilar to the other models, and substantially so for the high count locations, followed by Covid19Sim-Simulator. The one and two week horizons for the high count locations have practically identical dendrograms. The three week horizon one is very similar as well, save for the ordering of CU-select, JHUAPL-Gecko, and JHUAPL-Bucky, but CU-select and JHUAPL-Bucky have almost neglible difference in their Cramer's Distances. However, the shape of the dendrogram changes at the four week horizon for high count locations such that Covid19Sim-Simulator and CU-select are a separate branch together, even though these two models are farther apart for other horizons.  


The dendrograms for low count locations are different at each horizon. The one week horizon plot is similar to those for one to three week horizons of high count locations. Meanwhile, the two and three week horizon plots are resemble each other. However, the four week horizon dendrogram is different. At the same time, the scale for differences in Cramer Distance for the low count locations is very small, which may be a result of low hospitalizations, which may explain why there is so much variation in the dendrograms across horizons. 

For Saturday forecasts, Google_Harvard-CPF is consistently the most dissimilar from other models, followed by Covid19Sim-Simulator, across almost all horizons for both high-count and low count regions. This is the same as for Thursday forecasts. For Saturday forecasts, the heat maps seem to indicate models without day of the week effects tend to be more dissimilar from the COVIDhub-ensemble model but the dendrograms show less conclusive results. 


Day of the week effects may lead to a slight difference between models, but it does not seem to be a significant factor in explaining their differences. 
